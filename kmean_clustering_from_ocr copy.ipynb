{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "import PyPDF2\n",
    "from sklearn.metrics import silhouette_score\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "\n",
    "# If you don't have tesseract executable in your PATH, include the following:\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'source_docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(r'file_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df,df2, on=['ReferralName','ReferralCode','Source'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['FilePath']=df_merge['FilePath'].apply(lambda x: x.replace(\"Faxes\",\"Z:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = df_merge['FilePath']\n",
    "pdf_path = df_path.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8786\n"
     ]
    }
   ],
   "source": [
    "print(len(pdf_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform PDF to PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wand.image import Image\n",
    "import pytesseract\n",
    "\n",
    "# Set the desired resolution (DPI)\n",
    "dpi = 200\n",
    "\n",
    "destination = r'C:\\Users\\sanghoon.pai\\OneDrive - AdaptHealth\\Desktop\\sang-adapt-dojo\\bucket6_png'\n",
    "# Open the PDF file and change it to png\n",
    "for pt in pdf_path[1205:]:\n",
    "    with Image(filename=pt, resolution=dpi) as img:\n",
    "        file_name = destination + \"\\\\\" +os.path.basename(pt).split(\".\")[0] + \".png\"\n",
    "        try:\n",
    "            img.save(filename=file_name)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1205]\n"
     ]
    }
   ],
   "source": [
    "# code to get index where code being stopped and need to resume from the point\n",
    "pdf_name=''\n",
    "index = [i for i, path in enumerate(pdf_path) if pdf_name in path]\n",
    "print(index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# png_path = []\n",
    "# png_name = []\n",
    "\n",
    "# def list_files(path):\n",
    "#     for filename in os.listdir(path):\n",
    "#         file_path = os.path.join(path, filename)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             png_path.append(file_path)\n",
    "#             png_name.append(filename)\n",
    "\n",
    "# list_files(r\"C:\\Users\\sanghoon.pai\\OneDrive - AdaptHealth\\Desktop\\sang-adapt-dojo\\bucket6_png\")\n",
    "# png_dict = dict(zip(png_name, png_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(png, destination):\n",
    "    image = cv2.imread(png, cv2.IMREAD_GRAYSCALE)\n",
    "    _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # Deskew the image using the skew angle\n",
    "    coords = np.column_stack(np.where(binary_image > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle > 45:\n",
    "        angle = 90 - angle\n",
    "    else:\n",
    "        angle = -angle\n",
    "\n",
    "    (height, width) = binary_image.shape\n",
    "    center = (width // 2, height // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    deskewed_image = cv2.warpAffine(binary_image, M, (width, height), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    resized_image = cv2.resize(deskewed_image, (1920, 1920))\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    sharpened_image = cv2.filter2D(resized_image, -1, kernel)\n",
    "\n",
    "    file_name = destination + \"\\\\\" +os.path.basename(pt).split(\".\")[0] + \".png\"\n",
    "    # Save the pre-processed image\n",
    "    cv2.imwrite(file_name, sharpened_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination = r'C:\\Users\\sanghoon.pai\\OneDrive - AdaptHealth\\Desktop\\sang-adapt-dojo\\bucket6_png_processed'\n",
    "# for png in png_path:\n",
    "#     image_preprocessing(png,destination)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "png_path = []\n",
    "png_name = []\n",
    "\n",
    "def list_files(path):\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            png_path.append(file_path)\n",
    "            png_name.append(filename)\n",
    "list_files(r\"C:\\Users\\sanghoon.pai\\OneDrive - AdaptHealth\\Desktop\\sang-adapt-dojo\\bucket6_png\")\n",
    "png_dict = dict(zip(png_name, png_path))\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "full_text = {}\n",
    "for i, pt in enumerate(png_dict):\n",
    "    try:\n",
    "        image = Image.open(r\"C:\\Users\\sanghoon.pai\\OneDrive - AdaptHealth\\Desktop\\sang-adapt-dojo\\bucket6_png\" + pt)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        full_text[pt] = text\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('full_text_bucket6.json','w') as f:\n",
    "    json.dump(full_text,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.DataFrame.from_dict(png_dict , orient='index')\n",
    "df3=df3.reset_index()\n",
    "df3=df3.rename(columns={df3.columns[0]:'doc_name',df3.columns[1]: 'text'})\n",
    "df3['group'] = df3['doc_name'].str.split('-').str[0]\n",
    "result = df3.groupby('group').agg({'text': ' '.join})\n",
    "result= result.reset_index()\n",
    "result.columns = ['doc_name','text']\n",
    "result['doc_name'] = result['doc_name'].apply(lambda x: x + '.png' if not x.endswith('.png') else x)\n",
    "result.to_csv('bucket6.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK-Learn Kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(full_text)\n",
    "\n",
    "\n",
    "# Use elbow method to find the optimal number of clusters\n",
    "inertias = []\n",
    "silhouette_scores=[]\n",
    "for k in range(2, 2000,50):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(tfidf_matrix, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the elbow plot, choose the optimal number of clusters\n",
    "n_clusters = 700\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Use silhouette method to evaluate the clustering performance\n",
    "silhouette_avg = silhouette_score(tfidf_matrix, clusters)\n",
    "print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "# Create dataframe with clustering results\n",
    "\n",
    "\n",
    "cluster_pd = pd.DataFrame(clusters, columns = ['cluster'])\n",
    "name=pd.DataFrame(doc_name, columns = ['doc_name'])\n",
    "cluster_pd['doc_name']=doc_name\n",
    "\n",
    "cluster_pd.sort_values(by=[\"cluster\"], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_pd = pd.DataFrame(clusters, columns = ['cluster'])\n",
    "cluster_pd.sort_values(by=[\"cluster\"], inplace = True)\n",
    "cluster_merge = cluster_pd.merge(result,left_index=True, right_index=True,how='outer')\n",
    "cluster_merge.sort_values(by=[\"cluster\"], inplace = True)\n",
    "cluster_merge['doc_name'] = cluster_merge['doc_name'].str.replace( '.png','.pdf')\n",
    "df3.to_csv('train_cluster700.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(kmeans, \"kmeans_train_ocr.joblib\")\n",
    "\n",
    "import pickle\n",
    "with open(\"kmeans.pkl\", \"wb\") as file:\n",
    "    pickle.dump(kmeans, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
